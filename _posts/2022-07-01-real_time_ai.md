---
layout: post
title: NoCReAS - No to Concealed Real-time AI Support
date: 2022-07-01 00:00:00 +0300
description: An initiative for the regulation of real-time AI language models, to restrict their use on human-human interactions.
img: face_pexels-mohamed-abdelghaffar-771742.jpg 
tags: [Artificial Intelligence, Neural Network, Science]
---

Allow me to propose the initiative "No to Concealed Real-time AI Support" or NoCReAS for not-so-short-either. And what is NoCReAS you ask? Well, it's an incoming application of AI, affecting every social interaction you have along your day. Sounds promising, right?

Let's start with AI language models. Huge libraries of word and text properties, extracted from all sort of digital sources. These models are NOW generating indistinguishable text in any conversational situation. Soon they will provide better responses than us in certain domains.

Language models will probably be regulated like deep fakes in the AI Act. That is, notifying users when interacting with content generated by an AI, to avoid impersonation or deception. That will work fine for digital interactions, most likely though annoying forms.

But it won't stop there, cause there's a loophole. You can provide real-time AI language support to a knowing person, that's well and all. But what if this person uses such support to impersonate or deceive another one? That is, what if someone uses "concealed real-time AI support"?

Phones, watches, glasses, earphones, any other gadget. We are integrating our digital devices as an extension of us, and this will only grows stronger. At some point we will be able to percieve AI suggestions unknowingly to others. Concealed real-time AI support is unavoidable.

At first, you will constantly wonder who are you talking with. Was it Alex or the AI who made that joke? Eventually, you will not trust any of your interlocutors, their intent and sentiment. Open and fair conversations will be gone. Unfaithful, dubious interactions will remain.

To prevent that, NoCReAS seeks to regulate the use of language models in real-time interactions. (some) current regulation says, "people has the right to know if they are talking to an AI". Well, they should also know if they are "talking to an AI through a person".

In the case of deep fakes, regulation is struggling to catch up. For real-time human interactions let's fix it before it's broken. Cause it will be broken soon enough, and the damage to our society can be huge.

To apply such legislation, NoCReAS suggests to enforce a unique and non-compatible hardware & software stack on all projects, public or private, developing the enabling technology. This HW & SW will include an explicit disclaimer against any conconcealed real-time use.

